# MusicFM ðŸ¤–
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![License](https://img.shields.io/github/license/openshift/source-to-image.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)


**A Foundation Model for Music Informatics**, submitted to ICASSP 2024 [[paper](https://arxiv.org/abs/2311.03318)]

-- Minz Won, Yun-Ning Hung, and Duc Le 


## Quick start
### Download models
We share our model trained with the [FMA Dataset](https://github.com/mdeff/fma), which comprises 8k hours of Creative Common-licensed audio. While using a larger dataset (160k hours) can enhance performance, we've chosen to release the model trained on FMA to avoid potential licensing complications.

```
wget -P data/ https://huggingface.co/minzwon/MusicFM/resolve/main/fma_classic_stats.json
wget -P data/ https://huggingface.co/minzwon/MusicFM/resolve/main/musicfm_25hz_FMA_330m_500k.pt
```

### Get embeddings
```
import torch
from model.musicfm_25hz import MusicFM25Hz

# dummy audio
wav = (torch.rand(4, 24000 * 30) - 0.5) * 2

# load MusicFM
musicfm = MusicFM25Hz()

# to GPUs
wav = wav.cuda()
musicfm = musicfm.cuda()

# get embeddings
musicfm.eval()
emb = musicfm.get_latent(wav, layer_ix=6)
```

### Mixed precision and Flash attention
Suffering from memory issues? [Mixed precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html) and [Flash attention](https://arxiv.org/abs/2205.14135) will be good friends of yours!

```
import torch
from model.musicfm_25hz import MusicFM25Hz

# dummy audio
wav = (torch.rand(4, 24000 * 30) - 0.5) * 2

# load MusicFM
musicfm = MusicFM25Hz(is_flash=True)

# to GPUs
wav = wav.cuda().half()
musicfm = musicfm.cuda().half()

# get embeddings
musicfm.eval()
emb = musicfm.get_latent(wav, layer_ix=6)
```

### Usage in downstream tasks
The pretrained model operates at a 25Hz frame rate, but our downstream tasks demand varying temporal resolutions. To address this, we either summarize the sequence through global average pooling or adjust the temporal resolution using adaptive average pooling. 

```
# Sequence-level representation

# Token-level representation
```

## Results
<img src="figs/Table1.png" width="700">


## Masked token modeling
<img src="figs/Fig1.pdf" width="300">

MusicFM follows the training scheme of [BEST-RQ](https://arxiv.org/abs/2202.01855). Input audio is masked with noise, and the model predicts the masked representation. Target tokens are generated by random projection and a random codebook. Both the projection layer and codebook are **randomly initialized** and remain **non-trainable**. Isn't it fascinating?

Note that input normalization is exceptionally crucial, considering the usage of random projection. You can check the details [here](https://github.com/minzwon/musicfm/blob/d5d0f313add9f3c32c41f95521760b1a136809ed/model/musicfm_25hz.py#L148).

## Citation
```
@article{won2023musicfm,
    title={A Foundation Model for Music Informatics},
    author = {Won, Minz and Hung, Yun-Ning and Le, Duc},
    journal={arXiv preprint arXiv:2311.03318},
    year={2013}
}
```