# MusicFM ðŸ¤–
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![License](https://img.shields.io/github/license/openshift/source-to-image.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)


**A Foundation Model for Music Informatics**, submitted to ICASSP 2024 [[paper](https://arxiv.org/abs/2311.03318)]

-- Minz Won, Yun-Ning Hung, and Duc Le 


## Quick start
### Download models
We share our model trained with the [FMA Dataset](https://github.com/mdeff/fma), which comprises 8k hours of Creative Common-licensed audio. While using a larger dataset (160k hours) can enhance performance, we've chosen to release the model trained on FMA to avoid potential licensing complications.

```
wget -P data/ https://huggingface.co/minzwon/MusicFM/resolve/main/fma_classic_stats.json
wget -P data/ https://huggingface.co/minzwon/MusicFM/resolve/main/musicfm_25hz_FMA_330m_500k.pt
```

### Get embeddings
```
import torch
from model.musicfm_25hz import MusicFM25Hz

# dummy audio
wav = (torch.rand(4, 24000 * 30) - 0.5) * 2

# load MusicFM
musicfm = MusicFM25Hz()

# to GPUs
wav = wav.cuda()
musicfm = musicfm.cuda()

# get embeddings
musicfm.eval()
emb = musicfm.get_latent(wav, layer_ix=6)
```

### Mixed precision and Flash attention
Suffering from memory issues? [Mixed precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html) and [Flash attention](https://arxiv.org/abs/2205.14135) will be good friends of yours!

```
import torch
from model.musicfm_25hz import MusicFM25Hz

# dummy audio
wav = (torch.rand(4, 24000 * 30) - 0.5) * 2

# load MusicFM
musicfm = MusicFM25Hz(is_flash=True)

# to GPUs
wav = wav.cuda().half()
musicfm = musicfm.cuda().half()

# get embeddings
musicfm.eval()
emb = musicfm.get_latent(wav, layer_ix=6)
```

### Usage in downstream tasks
The pretrained model operates at a 25Hz frame rate, but our downstream tasks demand varying temporal resolutions. To address this, we either summarize the sequence through global average pooling or adjust the temporal resolution using adaptive average pooling. 

```
# Sequence-level representation

# Token-level representation
```

## Results

<img src="figs/Table1.png" width="800">

\* FM1 is pretrained [MERT](https://arxiv.org/abs/2306.00107). 

\*\*FM8 mirrors the [BEST-RQ](https://arxiv.org/abs/2202.01855) but with the distinction that it was trained using music data.


- Random tokenization generalizes well to music data. 

- Token-level classification offers a more comprehensive understanding of foundation models. While FM4 excels in music tagging, its performance in structural analysis is subpar.

- Input length used during training is critical for capturing
long-term contexts. Check 5s models (FM1, FM2, and FM4) and a 30s model (FM5) in downbeat tracking and structure analysis.

- Temporal resolution has less impact in our experimental setup. See FM5, FM6, and FM7.

- Model architecture makes a significant difference. Conformer (FM5) consistently outperformed BERT encoder (FM3) for across all downstream tasks. 

- The influence of model size was relatively minimal (FM7 and FM8). However, we observed that FM8's performance continued to improve, which is typically indicative of underfitting. All models were trained for two weeks to ensure a fair comparison.

- Data is undeniably crucial, as in any data-driven approach. Please compare FM7 and FM9.

- Fine-tuning the foundation model further enhances downstream performance. However, we did observe a performance
drop in the tagging task, primarily attributed to overfitting.

## Masked token modeling
<img src="figs/Fig1.png" width="300">

MusicFM follows the training scheme of [BEST-RQ](https://arxiv.org/abs/2202.01855). Input audio is masked with noise, and the model predicts the masked representation. Target tokens are generated by random projection and a random codebook. Both the projection layer and codebook are **randomly initialized** and remain **non-trainable**. Isn't it fascinating?

Note that input normalization is exceptionally crucial, considering the usage of random projection. You can check the details [here](https://github.com/minzwon/musicfm/blob/d5d0f313add9f3c32c41f95521760b1a136809ed/model/musicfm_25hz.py#L148).

## Citation
```
@article{won2023musicfm,
    title={A Foundation Model for Music Informatics},
    author = {Won, Minz and Hung, Yun-Ning and Le, Duc},
    journal={arXiv preprint arXiv:2311.03318},
    year={2013}
}
```